{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# Fine-Tunned YOLO Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using YOLO Pre-Trained Weights , we got an accuracy of arounf 83% which seems impressive but for the solution to be impressive , what we have done is to try to tune YOLO Model according to our dataset and use-case . Fine Tunning involves following operations;\n",
    "1. Freezing certain layers in order to prevent the overfitting\n",
    "2. Trying diffrent versions of YOLO Model \n",
    "3. Letting the weights evolve over generations \n",
    "4. Hyper-parameters tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Loading YOLO Model\n",
    "\n",
    "Clone GitHub [repository](https://github.com/ultralytics/yolov5), install [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) and check PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hG--yp-svIU0",
    "outputId": "7642be70-deb5-4beb-807d-d3d0350dec7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount(\"/content/gdrive\")\n",
    "cd /content/gdrive/MyDrive\n",
    "\n",
    "if not os.path.isdir(\"FYP1YOLOv5\"):\n",
    "    os.makedirs(\"FYP1YOLOv5\")\n",
    "    \n",
    "%cd FYP1YOLOv5\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt  # install\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "display = utils.notebook_init()  # checks\n",
    "pwd\n",
    "%cd yolov5\n",
    "cd /content/gdrive/MyDrive/FYP1YOLOv5/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY2VXXXu74w5"
   },
   "source": [
    "## 2. Training on Custom Dataset\n",
    "\n",
    "Training YOLOv5s model on our dataset\n",
    "\n",
    "- **Pretrained [Models](https://github.com/ultralytics/yolov5/tree/master/models)** are downloaded\n",
    "automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NcFxRcFdJ_O",
    "outputId": "818c54c9-81e8-4950-b9be-9efcb2c9bda9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=runs/train/exp16/weights/best.pt, cfg=, data=custom_data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 3 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "YOLOv5 üöÄ v7.0-53-g65071da Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 349/349 items from runs/train/exp16/weights/best.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 112.86it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 80/80 [00:01<00:00, 55.82it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.45 anchors/target, 0.996 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Plotting labels to runs/train/exp17/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp17\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/99      3.73G    0.04073    0.01256          0         26        640: 100% 114/114 [00:32<00:00,  3.49it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.15it/s]\n",
      "                   all         80        419      0.394      0.265      0.237     0.0855\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/99      4.63G    0.04015     0.0124          0         73        640: 100% 114/114 [00:29<00:00,  3.85it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.65it/s]\n",
      "                   all         80        419      0.365      0.265      0.222     0.0794\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/99      4.63G    0.04301    0.01286          0         38        640: 100% 114/114 [00:28<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.18it/s]\n",
      "                   all         80        419      0.367      0.222      0.192     0.0693\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/99      4.63G    0.04585    0.01374          0         16        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.51it/s]\n",
      "                   all         80        419      0.373       0.26      0.211      0.076\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/99      4.63G    0.04746    0.01452          0         78        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.74it/s]\n",
      "                   all         80        419      0.309      0.248      0.203     0.0759\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/99      4.63G     0.0462     0.0142          0         23        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.37it/s]\n",
      "                   all         80        419      0.362      0.227      0.198     0.0663\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/99      4.63G    0.04629    0.01398          0         83        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.54it/s]\n",
      "                   all         80        419      0.275      0.251      0.185     0.0681\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/99      4.63G    0.04686    0.01346          0         31        640: 100% 114/114 [00:33<00:00,  3.45it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.57it/s]\n",
      "                   all         80        419      0.402      0.227      0.206     0.0656\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/99      4.63G      0.048    0.01389          0         55        640: 100% 114/114 [00:29<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.05it/s]\n",
      "                   all         80        419      0.302      0.236      0.182     0.0582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       9/99      4.63G    0.04721    0.01374          0         38        640: 100% 114/114 [00:28<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.37it/s]\n",
      "                   all         80        419      0.285      0.232      0.182     0.0593\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/99      4.63G    0.04527    0.01412          0         89        640: 100% 114/114 [00:28<00:00,  3.94it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.62it/s]\n",
      "                   all         80        419      0.277      0.253      0.195     0.0727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/99      4.63G    0.04541    0.01485          0         66        640: 100% 114/114 [00:29<00:00,  3.87it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.99it/s]\n",
      "                   all         80        419       0.29      0.234      0.185     0.0603\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/99      4.63G    0.04537    0.01378          0         47        640: 100% 114/114 [00:29<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.63it/s]\n",
      "                   all         80        419      0.365      0.239      0.197     0.0664\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/99      4.63G    0.04443    0.01362          0         70        640: 100% 114/114 [00:29<00:00,  3.90it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.43it/s]\n",
      "                   all         80        419      0.288      0.232      0.181     0.0567\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/99      4.63G    0.04495    0.01363          0         50        640: 100% 114/114 [00:29<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.46it/s]\n",
      "                   all         80        419      0.297      0.205      0.172     0.0545\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/99      4.63G    0.04516    0.01512          0         68        640: 100% 114/114 [00:29<00:00,  3.87it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.62it/s]\n",
      "                   all         80        419      0.326       0.22      0.184     0.0677\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/99      4.63G    0.04278    0.01325          0         66        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.51it/s]\n",
      "                   all         80        419      0.363      0.212      0.173     0.0576\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      17/99      4.63G    0.04378    0.01313          0         11        640: 100% 114/114 [00:32<00:00,  3.54it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.84it/s]\n",
      "                   all         80        419      0.302      0.224      0.164     0.0516\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      18/99      4.63G    0.04313    0.01347          0         19        640: 100% 114/114 [00:28<00:00,  3.97it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.22it/s]\n",
      "                   all         80        419      0.312      0.255      0.207     0.0755\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/99      4.63G    0.04378    0.01287          0         65        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.68it/s]\n",
      "                   all         80        419      0.319      0.272      0.208     0.0758\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      20/99      4.63G    0.04271    0.01318          0         99        640: 100% 114/114 [00:28<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.76it/s]\n",
      "                   all         80        419      0.294      0.279      0.202     0.0722\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      21/99      4.63G    0.04254     0.0135          0         53        640: 100% 114/114 [00:29<00:00,  3.89it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.17it/s]\n",
      "                   all         80        419      0.337      0.229      0.183     0.0638\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      22/99      4.63G    0.04217    0.01348          0         50        640: 100% 114/114 [00:29<00:00,  3.86it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.96it/s]\n",
      "                   all         80        419      0.269      0.291      0.192      0.058\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      23/99      4.63G    0.04256    0.01342          0         60        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.70it/s]\n",
      "                   all         80        419      0.258       0.22      0.167     0.0535\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      24/99      4.63G     0.0416    0.01347          0         47        640: 100% 114/114 [00:29<00:00,  3.90it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.32it/s]\n",
      "                   all         80        419      0.388       0.26      0.225     0.0757\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      25/99      4.63G    0.04138    0.01278          0         37        640: 100% 114/114 [00:29<00:00,  3.89it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.12it/s]\n",
      "                   all         80        419      0.394      0.279      0.232     0.0774\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      26/99      4.63G    0.04027    0.01283          0         43        640: 100% 114/114 [00:29<00:00,  3.80it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.64it/s]\n",
      "                   all         80        419      0.344       0.27      0.237     0.0761\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      27/99      4.63G    0.04018    0.01236          0         22        640: 100% 114/114 [00:30<00:00,  3.78it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.01it/s]\n",
      "                   all         80        419      0.366      0.239      0.195     0.0669\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      28/99      4.63G    0.04106    0.01356          0         61        640: 100% 114/114 [00:29<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.74it/s]\n",
      "                   all         80        419       0.43       0.22      0.209     0.0708\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      29/99      4.63G    0.04149    0.01247          0         21        640: 100% 114/114 [00:29<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.94it/s]\n",
      "                   all         80        419      0.334      0.229      0.205     0.0657\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      30/99      4.63G    0.04124    0.01323          0         54        640: 100% 114/114 [00:29<00:00,  3.90it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.43it/s]\n",
      "                   all         80        419      0.307      0.239      0.193     0.0709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      31/99      4.63G    0.04102    0.01307          0         77        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.92it/s]\n",
      "                   all         80        419       0.45      0.205      0.193     0.0662\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      32/99      4.63G    0.04026    0.01222          0         61        640: 100% 114/114 [00:29<00:00,  3.85it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.58it/s]\n",
      "                   all         80        419      0.347      0.208      0.193     0.0688\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      33/99      4.63G    0.03979    0.01295          0         89        640: 100% 114/114 [00:29<00:00,  3.87it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.38it/s]\n",
      "                   all         80        419      0.284      0.261      0.203     0.0719\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      34/99      4.63G     0.0408    0.01281          0         46        640: 100% 114/114 [00:29<00:00,  3.85it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.70it/s]\n",
      "                   all         80        419      0.374      0.232      0.219     0.0728\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      35/99      4.63G    0.03965    0.01216          0         67        640: 100% 114/114 [00:29<00:00,  3.82it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.57it/s]\n",
      "                   all         80        419      0.295      0.215      0.172     0.0595\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      36/99      4.63G    0.04074    0.01284          0         82        640: 100% 114/114 [00:29<00:00,  3.83it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.53it/s]\n",
      "                   all         80        419      0.299      0.239      0.177     0.0594\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      37/99      4.63G    0.03891    0.01199          0         55        640: 100% 114/114 [00:32<00:00,  3.48it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.65it/s]\n",
      "                   all         80        419      0.409       0.22       0.21      0.073\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      38/99      4.63G    0.03889    0.01202          0         44        640: 100% 114/114 [00:29<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.16it/s]\n",
      "                   all         80        419      0.323      0.248        0.2     0.0607\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      39/99      4.63G    0.03946    0.01214          0         47        640: 100% 114/114 [00:28<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.99it/s]\n",
      "                   all         80        419      0.336      0.243       0.22     0.0771\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      40/99      4.63G    0.03842    0.01162          0         62        640: 100% 114/114 [00:28<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.84it/s]\n",
      "                   all         80        419      0.312       0.26      0.215     0.0801\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      41/99      4.63G    0.03793    0.01201          0         70        640: 100% 114/114 [00:28<00:00,  3.94it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.44it/s]\n",
      "                   all         80        419      0.309       0.26      0.195       0.07\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      42/99      4.63G    0.03839    0.01294          0         58        640: 100% 114/114 [00:29<00:00,  3.89it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.47it/s]\n",
      "                   all         80        419      0.342      0.265      0.226     0.0793\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      43/99      4.63G    0.03812    0.01244          0         48        640: 100% 114/114 [00:28<00:00,  3.94it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.28it/s]\n",
      "                   all         80        419      0.293      0.279      0.217     0.0849\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      44/99      4.63G    0.03798    0.01202          0         46        640: 100% 114/114 [00:28<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.55it/s]\n",
      "                   all         80        419      0.333      0.291       0.23     0.0791\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      45/99      4.63G     0.0386    0.01224          0         34        640: 100% 114/114 [00:29<00:00,  3.87it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.56it/s]\n",
      "                   all         80        419      0.372      0.248      0.224      0.082\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      46/99      4.63G    0.03785    0.01233          0         57        640: 100% 114/114 [00:29<00:00,  3.87it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.82it/s]\n",
      "                   all         80        419      0.391      0.196      0.193     0.0736\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      47/99      4.63G    0.03761    0.01155          0         53        640: 100% 114/114 [00:31<00:00,  3.66it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.33it/s]\n",
      "                   all         80        419      0.446      0.246      0.222     0.0796\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      48/99      4.63G    0.03805    0.01216          0         22        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.88it/s]\n",
      "                   all         80        419      0.309      0.255       0.19     0.0642\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      49/99      4.63G     0.0371     0.0119          0        126        640: 100% 114/114 [00:28<00:00,  3.97it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.59it/s]\n",
      "                   all         80        419      0.354       0.27      0.228     0.0859\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      50/99      4.63G    0.03732    0.01179          0         42        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.95it/s]\n",
      "                   all         80        419      0.309      0.243      0.201     0.0699\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      51/99      4.63G     0.0367    0.01103          0         48        640: 100% 114/114 [00:29<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.18it/s]\n",
      "                   all         80        419      0.362      0.296      0.251     0.0855\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      52/99      4.63G    0.03772    0.01145          0         35        640: 100% 114/114 [00:29<00:00,  3.90it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.58it/s]\n",
      "                   all         80        419      0.329      0.243      0.193     0.0671\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      53/99      4.63G    0.03678     0.0119          0         80        640: 100% 114/114 [00:28<00:00,  3.97it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.23it/s]\n",
      "                   all         80        419      0.306      0.243      0.204     0.0679\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      54/99      4.63G    0.03613    0.01105          0         74        640: 100% 114/114 [00:29<00:00,  3.89it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.53it/s]\n",
      "                   all         80        419      0.316      0.259      0.221     0.0793\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      55/99      4.63G    0.03757    0.01121          0         64        640: 100% 114/114 [00:28<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.14it/s]\n",
      "                   all         80        419       0.37       0.26      0.243     0.0869\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      56/99      4.63G    0.03636    0.01173          0         55        640: 100% 114/114 [00:29<00:00,  3.86it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.20it/s]\n",
      "                   all         80        419      0.334      0.236       0.21      0.077\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      57/99      4.63G    0.03505    0.01116          0         46        640: 100% 114/114 [00:28<00:00,  3.94it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.69it/s]\n",
      "                   all         80        419      0.312      0.239      0.194     0.0699\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      58/99      4.63G    0.03573    0.01083          0         36        640: 100% 114/114 [00:32<00:00,  3.55it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.70it/s]\n",
      "                   all         80        419      0.359      0.236      0.198     0.0717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      59/99      4.63G    0.03562    0.01161          0         56        640: 100% 114/114 [00:29<00:00,  3.89it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.66it/s]\n",
      "                   all         80        419       0.38      0.255      0.212      0.074\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      60/99      4.63G    0.03567    0.01143          0        103        640: 100% 114/114 [00:29<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.39it/s]\n",
      "                   all         80        419      0.311      0.283      0.212     0.0757\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      61/99      4.63G    0.03454    0.01102          0         79        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.38it/s]\n",
      "                   all         80        419      0.377      0.217      0.193     0.0701\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      62/99      4.63G    0.03701    0.01166          0        106        640: 100% 114/114 [00:29<00:00,  3.90it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.79it/s]\n",
      "                   all         80        419      0.357      0.253      0.228     0.0834\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      63/99      4.63G    0.03589    0.01175          0         33        640: 100% 114/114 [00:29<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  5.03it/s]\n",
      "                   all         80        419      0.394      0.241      0.235     0.0757\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      64/99      4.63G    0.03452    0.01114          0         43        640: 100% 114/114 [00:28<00:00,  3.94it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.62it/s]\n",
      "                   all         80        419      0.332      0.258      0.211     0.0734\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      65/99      4.63G    0.03634    0.01126          0         15        640: 100% 114/114 [00:29<00:00,  3.90it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.91it/s]\n",
      "                   all         80        419       0.38      0.255      0.225     0.0785\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      66/99      4.63G    0.03588    0.01151          0         96        640: 100% 114/114 [00:29<00:00,  3.90it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.43it/s]\n",
      "                   all         80        419      0.329      0.267      0.217     0.0749\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      67/99      4.63G    0.03594    0.01099          0         67        640: 100% 114/114 [00:29<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.33it/s]\n",
      "                   all         80        419      0.314      0.294      0.225      0.083\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      68/99      4.63G    0.03627    0.01136          0         51        640: 100% 114/114 [00:29<00:00,  3.85it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.68it/s]\n",
      "                   all         80        419      0.354      0.251      0.212     0.0739\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      69/99      4.63G    0.03563    0.01165          0         49        640: 100% 114/114 [00:30<00:00,  3.68it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.45it/s]\n",
      "                   all         80        419       0.33      0.227      0.198       0.07\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      70/99      4.63G    0.03549    0.01076          0         64        640: 100% 114/114 [00:29<00:00,  3.88it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.95it/s]\n",
      "                   all         80        419      0.398      0.265      0.229     0.0827\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      71/99      4.63G     0.0352    0.01085          0         26        640: 100% 114/114 [00:29<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  5.05it/s]\n",
      "                   all         80        419      0.304      0.234       0.19     0.0728\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      72/99      4.63G    0.03402     0.0111          0        112        640: 100% 114/114 [00:29<00:00,  3.89it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.15it/s]\n",
      "                   all         80        419      0.338      0.243      0.201     0.0718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      73/99      4.63G    0.03406    0.01105          0         93        640: 100% 114/114 [00:28<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.87it/s]\n",
      "                   all         80        419      0.327      0.239      0.206     0.0747\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      74/99      4.63G    0.03454    0.01132          0         24        640: 100% 114/114 [00:29<00:00,  3.87it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.40it/s]\n",
      "                   all         80        419      0.337      0.272      0.239     0.0846\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      75/99      4.63G    0.03424    0.01094          0         42        640: 100% 114/114 [00:29<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.51it/s]\n",
      "                   all         80        419      0.325      0.248      0.212     0.0786\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      76/99      4.63G    0.03402    0.01107          0         58        640: 100% 114/114 [00:29<00:00,  3.88it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.73it/s]\n",
      "                   all         80        419      0.287      0.272       0.21     0.0749\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      77/99      4.63G    0.03329    0.01113          0         55        640: 100% 114/114 [00:29<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.29it/s]\n",
      "                   all         80        419      0.324      0.246      0.207     0.0807\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      78/99      4.63G     0.0338    0.01092          0         64        640: 100% 114/114 [00:29<00:00,  3.90it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  5.01it/s]\n",
      "                   all         80        419      0.381      0.232      0.221     0.0753\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      79/99      4.63G    0.03334    0.01071          0         93        640: 100% 114/114 [00:32<00:00,  3.55it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.27it/s]\n",
      "                   all         80        419      0.385      0.258      0.229     0.0759\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      80/99      4.63G    0.03432    0.01068          0         38        640: 100% 114/114 [00:28<00:00,  3.94it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.01it/s]\n",
      "                   all         80        419      0.305      0.258      0.207     0.0777\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      81/99      4.63G    0.03359    0.01045          0         76        640: 100% 114/114 [00:28<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.40it/s]\n",
      "                   all         80        419      0.414       0.21      0.205     0.0756\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      82/99      4.63G    0.03308    0.01094          0         32        640: 100% 114/114 [00:28<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.93it/s]\n",
      "                   all         80        419      0.296      0.248        0.2     0.0748\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      83/99      4.63G    0.03316    0.01065          0         56        640: 100% 114/114 [00:29<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.60it/s]\n",
      "                   all         80        419      0.366      0.251      0.229     0.0799\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      84/99      4.63G    0.03331    0.01049          0         31        640: 100% 114/114 [00:28<00:00,  3.94it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.15it/s]\n",
      "                   all         80        419      0.305      0.224      0.199     0.0729\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      85/99      4.63G    0.03307    0.01103          0         60        640: 100% 114/114 [00:28<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.56it/s]\n",
      "                   all         80        419      0.358      0.236      0.217     0.0757\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      86/99      4.63G    0.03315    0.01079          0         38        640: 100% 114/114 [00:29<00:00,  3.89it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.87it/s]\n",
      "                   all         80        419      0.428       0.22      0.223     0.0803\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      87/99      4.63G    0.03193   0.009777          0         18        640: 100% 114/114 [00:28<00:00,  4.00it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.11it/s]\n",
      "                   all         80        419      0.461      0.227      0.222     0.0802\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      88/99      4.63G     0.0325     0.0107          0         65        640: 100% 114/114 [00:29<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.79it/s]\n",
      "                   all         80        419      0.435      0.229      0.217     0.0796\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      89/99      4.63G    0.03191     0.0101          0         87        640: 100% 114/114 [00:28<00:00,  4.00it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.46it/s]\n",
      "                   all         80        419       0.39      0.253      0.221     0.0783\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      90/99      4.63G    0.03249    0.01073          0         89        640: 100% 114/114 [00:31<00:00,  3.58it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.47it/s]\n",
      "                   all         80        419      0.353      0.263      0.227     0.0761\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      91/99      4.63G    0.03235   0.009892          0         48        640: 100% 114/114 [00:28<00:00,  3.97it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.51it/s]\n",
      "                   all         80        419      0.337      0.267      0.208       0.08\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      92/99      4.63G    0.03203    0.01069          0         48        640: 100% 114/114 [00:29<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.22it/s]\n",
      "                   all         80        419      0.375      0.241       0.21     0.0785\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      93/99      4.63G    0.03188     0.0101          0         42        640: 100% 114/114 [00:29<00:00,  3.90it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.16it/s]\n",
      "                   all         80        419      0.318      0.243      0.203     0.0754\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      94/99      4.63G    0.03155    0.01023          0         38        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.88it/s]\n",
      "                   all         80        419      0.355      0.241       0.21     0.0764\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      95/99      4.63G    0.03177    0.01061          0         47        640: 100% 114/114 [00:29<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.72it/s]\n",
      "                   all         80        419        0.4      0.232      0.212     0.0775\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      96/99      4.63G    0.03187    0.01052          0         65        640: 100% 114/114 [00:28<00:00,  3.94it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.81it/s]\n",
      "                   all         80        419      0.402      0.212      0.203     0.0735\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      97/99      4.63G    0.03218     0.0107          0         52        640: 100% 114/114 [00:29<00:00,  3.90it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.77it/s]\n",
      "                   all         80        419      0.375      0.224      0.215     0.0773\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      98/99      4.63G    0.03153   0.009919          0         20        640: 100% 114/114 [00:28<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.95it/s]\n",
      "                   all         80        419      0.349      0.251      0.216     0.0811\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      99/99      4.63G    0.03211   0.009855          0         49        640: 100% 114/114 [00:29<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  4.51it/s]\n",
      "                   all         80        419       0.36      0.253      0.215     0.0782\n",
      "\n",
      "100 epochs completed in 0.848 hours.\n",
      "Optimizer stripped from runs/train/exp17/weights/last.pt, 14.4MB\n",
      "Optimizer stripped from runs/train/exp17/weights/best.pt, 14.4MB\n",
      "\n",
      "Validating runs/train/exp17/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.79it/s]\n",
      "                   all         80        419      0.367       0.26      0.243     0.0868\n",
      "Results saved to \u001b[1mruns/train/exp17\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv5s on COCO128 for 3 epochs\n",
    "!python train.py --img 640 --batch 16 --epochs 100 --data custom_data.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_qRrvF_t2a6",
    "outputId": "d0d0c7b9-0666-4cd6-c5ce-96947bd44b95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=runs/train/exp16/weights/best.pt, cfg=, data=custom_data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=300, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 3 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "YOLOv5 üöÄ v7.0-53-g65071da Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=3\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=3\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt to yolov5n.pt...\n",
      "100% 3.87M/3.87M [00:00<00:00, 113MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:17<00:00, 106.52it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.55 anchors/target, 0.168 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7519: 100% 1000/1000 [00:03<00:00, 323.70it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9961 best possible recall, 3.74 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.276/0.753-mean/best, past_thr=0.543-mean: 14,11, 24,18, 36,29, 55,49, 183,204, 173,378, 430,180, 302,304, 416,428\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.67G    0.05351    0.01184          0         26        640: 100% 114/114 [00:33<00:00,  3.38it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.67G    0.04236    0.01141          0         73        640: 100% 114/114 [00:30<00:00,  3.69it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.67G    0.04255    0.01128          0         38        640: 100% 114/114 [00:30<00:00,  3.77it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.67G    0.04269    0.01182          0         16        640: 100% 114/114 [00:30<00:00,  3.77it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.67G    0.04365    0.01254          0         78        640: 100% 114/114 [00:30<00:00,  3.74it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.67G    0.04158    0.01209          0         23        640: 100% 114/114 [00:30<00:00,  3.74it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.67G    0.04152    0.01203          0         83        640: 100% 114/114 [00:30<00:00,  3.76it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.67G    0.04057    0.01128          0         31        640: 100% 114/114 [00:30<00:00,  3.77it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.67G    0.04025    0.01138          0         55        640: 100% 114/114 [00:34<00:00,  3.30it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.67G    0.03734      0.011          0         38        640: 100% 114/114 [00:30<00:00,  3.76it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.93it/s]\n",
      "                   all         80        419      0.289      0.248      0.176     0.0632\n",
      "\n",
      "10 epochs completed in 0.087 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m1 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.28861,              0.24821,              0.17574,             0.063247,             0.056415,             0.025316,                    0,                 0.01,                 0.01,                0.937,               0.0005,                    3,                  0.8,                  0.1,                 0.05,                  0.5,                    1,                    1,                    1,                  0.2,                    4,                    0,                0.015,                  0.7,                  0.4,                    0,                  0.1,                  0.5,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    3\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01026, lrf=0.01008, momentum=0.93375, weight_decay=0.0005, warmup_epochs=3.03589, warmup_momentum=0.8, warmup_bias_lr=0.10172, box=0.05034, cls=0.49794, cls_pw=0.96521, obj=0.98885, obj_pw=0.99389, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.01541, hsv_s=0.66942, hsv_v=0.4, degrees=0.0, translate=0.09679, scale=0.49416, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.97472\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.97472\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01026) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:17<00:00, 105.29it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.55 anchors/target, 0.168 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7519: 100% 1000/1000 [00:02<00:00, 340.04it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9961 best possible recall, 3.74 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.276/0.753-mean/best, past_thr=0.543-mean: 14,11, 24,18, 36,29, 55,49, 183,204, 173,378, 430,180, 302,304, 416,428\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.68G    0.05395     0.0117          0         26        640: 100% 114/114 [00:35<00:00,  3.22it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.68G    0.04226    0.01124          0         70        640: 100% 114/114 [00:32<00:00,  3.46it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.68G    0.04214    0.01113          0         38        640: 100% 114/114 [00:32<00:00,  3.55it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.68G    0.04237    0.01186          0         16        640: 100% 114/114 [00:32<00:00,  3.52it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.68G    0.04335    0.01219          0         78        640: 100% 114/114 [00:31<00:00,  3.57it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.68G    0.04218    0.01189          0         23        640: 100% 114/114 [00:34<00:00,  3.35it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.68G    0.04202    0.01173          0         83        640: 100% 114/114 [00:34<00:00,  3.26it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.68G    0.04086    0.01116          0         31        640: 100% 114/114 [00:32<00:00,  3.48it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.68G    0.04054    0.01129          0         55        640: 100% 114/114 [00:31<00:00,  3.56it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.68G    0.03807     0.0108          0         41        640: 100% 114/114 [00:32<00:00,  3.56it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.62it/s]\n",
      "                   all         80        419      0.271      0.248      0.173     0.0648\n",
      "\n",
      "10 epochs completed in 0.092 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m2 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m              0.2714,              0.24821,              0.17325,             0.064838,             0.055661,             0.026229,                    0,              0.01026,              0.01008,              0.93375,               0.0005,               3.0359,                  0.8,              0.10172,              0.05034,              0.49794,              0.96521,              0.98885,              0.99389,                  0.2,                    4,                    0,              0.01541,              0.66942,                  0.4,                    0,              0.09679,              0.49416,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.9747\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01026, lrf=0.01053, momentum=0.89137, weight_decay=0.0005, warmup_epochs=3.50848, warmup_momentum=0.7451, warmup_bias_lr=0.1166, box=0.05034, cls=0.33419, cls_pw=1.16954, obj=1.2148, obj_pw=0.77329, iou_t=0.2, anchor_t=4.08178, fl_gamma=0.0, hsv_h=0.01572, hsv_s=0.67962, hsv_v=0.50506, degrees=0.0, translate=0.12815, scale=0.49416, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.78595, mixup=0.0, copy_paste=0.0, anchors=2.9747\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.9747\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01026) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:17<00:00, 105.77it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.58 anchors/target, 0.176 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7519: 100% 1000/1000 [00:02<00:00, 363.72it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.24: 0.9964 best possible recall, 3.75 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.274/0.753-mean/best, past_thr=0.540-mean: 14,12, 25,18, 37,29, 58,50, 182,218, 395,163, 173,403, 308,304, 420,435\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.69G    0.05498    0.01175          0         36        640: 100% 114/114 [00:32<00:00,  3.54it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.69G    0.04488    0.01093          0         25        640: 100% 114/114 [00:30<00:00,  3.73it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.69G    0.04135    0.01081          0         70        640: 100% 114/114 [00:30<00:00,  3.76it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.69G     0.0405    0.01121          0         23        640: 100% 114/114 [00:34<00:00,  3.31it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.69G    0.04115    0.01116          0         53        640: 100% 114/114 [00:30<00:00,  3.71it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.69G    0.04095    0.01175          0         31        640: 100% 114/114 [00:31<00:00,  3.66it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.69G    0.03914    0.01135          0         25        640: 100% 114/114 [00:30<00:00,  3.71it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.69G    0.03918      0.011          0         76        640: 100% 114/114 [00:30<00:00,  3.70it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.69G    0.03672    0.01083          0         20        640: 100% 114/114 [00:30<00:00,  3.74it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.69G    0.03788    0.01134          0         35        640: 100% 114/114 [00:30<00:00,  3.74it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.58it/s]\n",
      "                   all         80        419      0.374      0.284      0.226     0.0759\n",
      "\n",
      "10 epochs completed in 0.087 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m3 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.37431,              0.28401,              0.22581,             0.075893,              0.05586,             0.025321,                    0,              0.01026,              0.01053,              0.89137,               0.0005,               3.5085,               0.7451,               0.1166,              0.05034,              0.33419,               1.1695,               1.2148,              0.77329,                  0.2,               4.0818,                    0,              0.01572,              0.67962,              0.50506,                    0,              0.12815,              0.49416,                    0,                    0,                    0,                  0.5,              0.78595,                    0,                    0,               2.9747\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01026, lrf=0.01, momentum=0.93546, weight_decay=0.0005, warmup_epochs=3.08501, warmup_momentum=0.80343, warmup_bias_lr=0.10199, box=0.05004, cls=0.49794, cls_pw=0.96521, obj=1.01874, obj_pw=0.99927, iou_t=0.2, anchor_t=4.06238, fl_gamma=0.0, hsv_h=0.01524, hsv_s=0.66507, hsv_v=0.403, degrees=0.0, translate=0.09701, scale=0.49596, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.85149\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.85149\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01026) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 110.43it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.57 anchors/target, 0.174 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7518: 100% 1000/1000 [00:02<00:00, 351.05it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9961 best possible recall, 3.77 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.276/0.752-mean/best, past_thr=0.539-mean: 14,11, 24,18, 37,29, 56,50, 174,180, 388,184, 188,391, 315,314, 430,425\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.68G    0.05354    0.01215          0         26        640: 100% 114/114 [00:38<00:00,  2.99it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.68G    0.04155    0.01172          0         71        640: 100% 114/114 [00:33<00:00,  3.36it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.68G    0.04154    0.01158          0         38        640: 100% 114/114 [00:33<00:00,  3.45it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.68G    0.04313    0.01229          0         16        640: 100% 114/114 [00:34<00:00,  3.33it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.68G    0.04383    0.01288          0         78        640: 100% 114/114 [00:34<00:00,  3.34it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.68G    0.04135     0.0123          0         23        640: 100% 114/114 [00:37<00:00,  3.03it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.68G    0.04152    0.01223          0         83        640: 100% 114/114 [00:33<00:00,  3.38it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.68G    0.04065    0.01165          0         31        640: 100% 114/114 [00:34<00:00,  3.35it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.68G    0.03999    0.01173          0         55        640: 100% 114/114 [00:33<00:00,  3.40it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.68G    0.03689    0.01138          0         41        640: 100% 114/114 [00:33<00:00,  3.44it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.72it/s]\n",
      "                   all         80        419      0.346      0.251      0.213     0.0759\n",
      "\n",
      "10 epochs completed in 0.096 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m4 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.34612,              0.25141,              0.21289,             0.075915,             0.055661,             0.027782,                    0,              0.01026,                 0.01,              0.93546,               0.0005,                3.085,              0.80343,              0.10199,              0.05004,              0.49794,              0.96521,               1.0187,              0.99927,                  0.2,               4.0624,                    0,              0.01524,              0.66507,                0.403,                    0,              0.09701,              0.49596,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.8515\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01072, lrf=0.01, momentum=0.98, weight_decay=0.00052, warmup_epochs=3.28369, warmup_momentum=0.77881, warmup_bias_lr=0.08992, box=0.04285, cls=0.49794, cls_pw=1.14993, obj=1.05587, obj_pw=1.07323, iou_t=0.2, anchor_t=3.47872, fl_gamma=0.0, hsv_h=0.01185, hsv_s=0.64589, hsv_v=0.4, degrees=0.0, translate=0.07739, scale=0.47639, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.98766\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.98766\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01072) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00052), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 111.74it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.36 anchors/target, 0.113 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7513: 100% 1000/1000 [00:03<00:00, 331.04it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.29: 0.9927 best possible recall, 3.50 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.275/0.753-mean/best, past_thr=0.559-mean: 15,12, 25,18, 37,29, 55,51, 169,194, 411,161, 169,415, 312,310, 411,431\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.68G    0.04587    0.01197          0         26        640: 100% 114/114 [00:38<00:00,  2.97it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.68G    0.03529    0.01142          0         67        640: 100% 114/114 [00:33<00:00,  3.44it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.68G    0.03385    0.01152          0         36        640: 100% 114/114 [00:32<00:00,  3.48it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.68G    0.03687     0.0121          0         15        640: 100% 114/114 [00:32<00:00,  3.47it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.68G    0.03963    0.01373          0         78        640: 100% 114/114 [00:33<00:00,  3.41it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.68G    0.03926    0.01404          0         23        640: 100% 114/114 [00:33<00:00,  3.44it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.68G    0.04167    0.01407          0         80        640: 100% 114/114 [00:37<00:00,  3.01it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.68G    0.03965    0.01359          0         30        640: 100% 114/114 [00:33<00:00,  3.41it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.68G    0.03967    0.01392          0         53        640: 100% 114/114 [00:33<00:00,  3.43it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.68G    0.03585    0.01327          0         42        640: 100% 114/114 [00:33<00:00,  3.39it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.68it/s]\n",
      "                   all         80        419      0.345       0.22      0.183     0.0601\n",
      "\n",
      "10 epochs completed in 0.095 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m5 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.34518,              0.21957,               0.1828,             0.060133,             0.048681,             0.028716,                    0,              0.01072,                 0.01,                 0.98,              0.00052,               3.2837,              0.77881,              0.08992,              0.04285,              0.49794,               1.1499,               1.0559,               1.0732,                  0.2,               3.4787,                    0,              0.01185,              0.64589,                  0.4,                    0,              0.07739,              0.47639,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.9877\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01054, lrf=0.01, momentum=0.98, weight_decay=0.00041, warmup_epochs=2.87512, warmup_momentum=0.86974, warmup_bias_lr=0.11571, box=0.05, cls=0.51012, cls_pw=0.97983, obj=0.91718, obj_pw=0.74704, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.01256, hsv_s=0.59595, hsv_v=0.32493, degrees=0.0, translate=0.10806, scale=0.39212, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.68472\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.68472\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01054) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00041), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 110.32it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.55 anchors/target, 0.168 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7519: 100% 1000/1000 [00:02<00:00, 339.30it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9961 best possible recall, 3.74 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.276/0.753-mean/best, past_thr=0.543-mean: 14,11, 24,18, 36,29, 55,49, 183,204, 173,378, 430,180, 302,304, 416,428\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.68G    0.05032   0.008643          0         22        640: 100% 114/114 [00:34<00:00,  3.28it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.68G    0.04152   0.008419          0         62        640: 100% 114/114 [00:33<00:00,  3.44it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.68G     0.0438     0.0084          0         26        640: 100% 114/114 [00:36<00:00,  3.12it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.68G    0.04802   0.009425          0         14        640: 100% 114/114 [00:32<00:00,  3.45it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.68G    0.05221    0.01058          0         71        640: 100% 114/114 [00:32<00:00,  3.48it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.68G    0.05109    0.01048          0         20        640: 100% 114/114 [00:32<00:00,  3.47it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.68G    0.05352    0.01067          0         78        640: 100% 114/114 [00:32<00:00,  3.46it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.68G    0.05065    0.01007          0         35        640: 100% 114/114 [00:33<00:00,  3.39it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.68G    0.04916    0.01026          0         42        640: 100% 114/114 [00:33<00:00,  3.37it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.68G    0.04486   0.009876          0         29        640: 100% 114/114 [00:34<00:00,  3.35it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.68it/s]\n",
      "                   all         80        419      0.349      0.205      0.173     0.0595\n",
      "\n",
      "10 epochs completed in 0.094 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m6 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.34907,              0.20525,               0.1726,              0.05947,             0.057514,             0.020224,                    0,              0.01054,                 0.01,                 0.98,              0.00041,               2.8751,              0.86974,              0.11571,                 0.05,              0.51012,              0.97983,              0.91718,              0.74704,                  0.2,                    4,                    0,              0.01256,              0.59595,              0.32493,                    0,              0.10806,              0.39212,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.6847\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00652, lrf=0.01026, momentum=0.88453, weight_decay=0.00042, warmup_epochs=3.30209, warmup_momentum=0.7715, warmup_bias_lr=0.10551, box=0.03635, cls=0.57137, cls_pw=0.91913, obj=0.86947, obj_pw=1.13496, iou_t=0.2, anchor_t=4.37743, fl_gamma=0.0, hsv_h=0.01431, hsv_s=0.72421, hsv_v=0.4, degrees=0.0, translate=0.09627, scale=0.49383, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.91442, mixup=0.0, copy_paste=0.0, anchors=2.14048\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.14048\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7016932 parameters, 7016932 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 342/349 items from runs/train/exp16/weights/best.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00652) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00042), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:18<00:00, 99.81it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.07 anchors/target, 0.023 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7202: 100% 1000/1000 [00:03<00:00, 290.59it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.23: 0.9955 best possible recall, 2.79 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=640, metric_all=0.294/0.721-mean/best, past_thr=0.543-mean: 15,12, 28,21, 47,40, 242,302, 421,183, 403,402\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.67G    0.06129    0.02708          0         40        640: 100% 114/114 [00:32<00:00,  3.46it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.67G    0.04351    0.02329          0         60        640: 100% 114/114 [00:31<00:00,  3.64it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.67G     0.0366    0.01543          0         57        640: 100% 114/114 [00:30<00:00,  3.73it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.67G    0.03593    0.01343          0         57        640: 100% 114/114 [00:30<00:00,  3.78it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.67G    0.03399    0.01295          0         42        640: 100% 114/114 [00:30<00:00,  3.78it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.67G    0.03226    0.01301          0         53        640: 100% 114/114 [00:30<00:00,  3.78it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.67G    0.03035    0.01162          0         55        640: 100% 114/114 [00:30<00:00,  3.76it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.67G    0.03018    0.01173          0         54        640: 100% 114/114 [00:30<00:00,  3.74it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.67G    0.02944    0.01117          0         57        640: 100% 114/114 [00:34<00:00,  3.31it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.67G    0.02895    0.01112          0         31        640: 100% 114/114 [00:30<00:00,  3.79it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.98it/s]\n",
      "                   all         80        419      0.325      0.282       0.22     0.0752\n",
      "\n",
      "10 epochs completed in 0.087 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m7 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m              0.3247,              0.28162,              0.21966,             0.075198,             0.041806,             0.025665,                    0,              0.00652,              0.01026,              0.88453,              0.00042,               3.3021,               0.7715,              0.10551,              0.03635,              0.57137,              0.91913,              0.86947,                1.135,                  0.2,               4.3774,                    0,              0.01431,              0.72421,                  0.4,                    0,              0.09627,              0.49383,                    0,                    0,                    0,                  0.5,              0.91442,                    0,                    0,               2.1405\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01169, lrf=0.01008, momentum=0.86114, weight_decay=0.0005, warmup_epochs=3.0359, warmup_momentum=0.85324, warmup_bias_lr=0.10172, box=0.05034, cls=0.49794, cls_pw=0.92072, obj=1.05001, obj_pw=0.99389, iou_t=0.2, anchor_t=4.4173, fl_gamma=0.0, hsv_h=0.01263, hsv_s=0.82194, hsv_v=0.32171, degrees=0.0, translate=0.10376, scale=0.67302, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.8762, mixup=0.0, copy_paste=0.0, anchors=2.9747\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.9747\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01169) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 110.40it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.71 anchors/target, 0.211 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7522: 100% 1000/1000 [00:02<00:00, 349.49it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.23: 0.9968 best possible recall, 3.90 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.276/0.753-mean/best, past_thr=0.530-mean: 15,12, 24,18, 37,29, 56,50, 175,190, 415,171, 193,375, 321,323, 426,429\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.68G    0.05702    0.01311          0         30        640: 100% 114/114 [00:32<00:00,  3.48it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.68G    0.04537    0.01228          0         43        640: 100% 114/114 [00:31<00:00,  3.65it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.68G    0.04636    0.01321          0         47        640: 100% 114/114 [00:30<00:00,  3.71it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.68G    0.04664     0.0126          0         60        640: 100% 114/114 [00:30<00:00,  3.70it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.68G    0.04665    0.01332          0         74        640: 100% 114/114 [00:31<00:00,  3.65it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.68G    0.04494     0.0131          0         40        640: 100% 114/114 [00:35<00:00,  3.25it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.68G    0.04246    0.01278          0         36        640: 100% 114/114 [00:30<00:00,  3.72it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.68G    0.04097    0.01234          0         40        640: 100% 114/114 [00:30<00:00,  3.70it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.68G    0.03979    0.01211          0         33        640: 100% 114/114 [00:30<00:00,  3.71it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.68G    0.04079    0.01225          0         50        640: 100% 114/114 [00:30<00:00,  3.74it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.86it/s]\n",
      "                   all         80        419      0.356      0.236      0.198     0.0731\n",
      "\n",
      "10 epochs completed in 0.088 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m8 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.35571,              0.23628,              0.19801,             0.073053,             0.056652,             0.026212,                    0,              0.01169,              0.01008,              0.86114,               0.0005,               3.0359,              0.85324,              0.10172,              0.05034,              0.49794,              0.92072,                 1.05,              0.99389,                  0.2,               4.4173,                    0,              0.01263,              0.82194,              0.32171,                    0,              0.10376,              0.67302,                    0,                    0,                    0,                  0.5,               0.8762,                    0,                    0,               2.9747\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01251, lrf=0.01, momentum=0.8727, weight_decay=0.0005, warmup_epochs=2.97008, warmup_momentum=0.86067, warmup_bias_lr=0.10209, box=0.05018, cls=0.50166, cls_pw=0.89611, obj=0.98375, obj_pw=0.96636, iou_t=0.2, anchor_t=4.66186, fl_gamma=0.0, hsv_h=0.01169, hsv_s=0.82755, hsv_v=0.32195, degrees=0.0, translate=0.10376, scale=0.68475, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.95001, mixup=0.0, copy_paste=0.0, anchors=2.9574\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.9574\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01251) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 112.17it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.80 anchors/target, 0.235 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7523: 100% 1000/1000 [00:02<00:00, 354.80it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.21: 0.9968 best possible recall, 3.99 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.275/0.753-mean/best, past_thr=0.522-mean: 15,12, 25,18, 37,30, 55,50, 172,191, 175,402, 435,178, 300,309, 432,428\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.69G    0.05735    0.01304          0         22        640: 100% 114/114 [00:32<00:00,  3.46it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.69G    0.04664    0.01231          0         72        640: 100% 114/114 [00:31<00:00,  3.66it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.69G     0.0492     0.0126          0         41        640: 100% 114/114 [00:34<00:00,  3.29it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.69G    0.04736    0.01265          0         49        640: 100% 114/114 [00:31<00:00,  3.65it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.69G    0.04676    0.01235          0         55        640: 100% 114/114 [00:30<00:00,  3.72it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.69G    0.04488    0.01266          0         46        640: 100% 114/114 [00:31<00:00,  3.67it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.69G    0.04393    0.01252          0         79        640: 100% 114/114 [00:30<00:00,  3.70it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.69G     0.0428    0.01192          0         62        640: 100% 114/114 [00:30<00:00,  3.74it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.69G    0.04206    0.01176          0         36        640: 100% 114/114 [00:30<00:00,  3.76it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.69G    0.04101    0.01112          0         68        640: 100% 114/114 [00:30<00:00,  3.74it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:00<00:00,  3.41it/s]\n",
      "                   all         80        419      0.399      0.269      0.217     0.0743\n",
      "\n",
      "10 epochs completed in 0.087 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m9 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.39883,              0.26917,               0.2166,              0.07432,             0.060635,             0.024899,                    0,              0.01251,                 0.01,               0.8727,               0.0005,               2.9701,              0.86067,              0.10209,              0.05018,              0.50166,              0.89611,              0.98375,              0.96636,                  0.2,               4.6619,                    0,              0.01169,              0.82755,              0.32195,                    0,              0.10376,              0.68475,                    0,                    0,                    0,                  0.5,              0.95001,                    0,                    0,               2.9574\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01054, lrf=0.01024, momentum=0.86998, weight_decay=0.00059, warmup_epochs=2.9701, warmup_momentum=0.48037, warmup_bias_lr=0.08128, box=0.0531, cls=0.42838, cls_pw=1.02216, obj=1.28025, obj_pw=1.09184, iou_t=0.2, anchor_t=5.57908, fl_gamma=0.0, hsv_h=0.01233, hsv_s=0.9, hsv_v=0.1985, degrees=0.0, translate=0.11042, scale=0.75186, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.9574\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.9574\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01054) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00059), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 111.01it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m1.12 anchors/target, 0.318 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7528: 100% 1000/1000 [00:03<00:00, 264.01it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.18: 0.9987 best possible recall, 4.30 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.275/0.753-mean/best, past_thr=0.498-mean: 15,12, 24,18, 37,30, 56,50, 168,179, 175,399, 442,161, 307,308, 418,432\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.69G     0.0708    0.01946          0         35        640: 100% 114/114 [00:35<00:00,  3.24it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.69G    0.05491    0.01921          0         84        640: 100% 114/114 [00:32<00:00,  3.46it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.69G    0.05322    0.01872          0         52        640: 100% 114/114 [00:33<00:00,  3.44it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.69G    0.05352    0.01914          0         28        640: 100% 114/114 [00:32<00:00,  3.46it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.69G    0.05347    0.01986          0         81        640: 100% 114/114 [00:33<00:00,  3.43it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.69G    0.05058    0.01891          0         29        640: 100% 114/114 [00:32<00:00,  3.47it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.69G    0.04942    0.01834          0         81        640: 100% 114/114 [00:36<00:00,  3.11it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.69G    0.04973    0.01782          0         29        640: 100% 114/114 [00:32<00:00,  3.46it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.69G    0.04846      0.018          0         52        640: 100% 114/114 [00:32<00:00,  3.46it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.69G    0.04729    0.01749          0         42        640: 100% 114/114 [00:32<00:00,  3.46it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.50it/s]\n",
      "                   all         80        419      0.332      0.272      0.214      0.072\n",
      "\n",
      "10 epochs completed in 0.094 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m10 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.33206,              0.27208,               0.2137,             0.072007,             0.072671,             0.038977,                    0,              0.01054,              0.01024,              0.86998,              0.00059,               2.9701,              0.48037,              0.08128,               0.0531,              0.42838,               1.0222,               1.2803,               1.0918,                  0.2,               5.5791,                    0,              0.01233,                  0.9,               0.1985,                    0,              0.11042,              0.75186,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.9574\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01335, lrf=0.01, momentum=0.82845, weight_decay=0.00059, warmup_epochs=3.19581, warmup_momentum=0.86067, warmup_bias_lr=0.09543, box=0.05162, cls=0.50471, cls_pw=0.99833, obj=0.90884, obj_pw=1.05576, iou_t=0.2, anchor_t=4.6619, fl_gamma=0.0, hsv_h=0.01093, hsv_s=0.89837, hsv_v=0.27939, degrees=0.0, translate=0.1101, scale=0.66407, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.99551\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.99551\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01335) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00059), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 111.49it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.80 anchors/target, 0.235 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7523: 100% 1000/1000 [00:02<00:00, 364.88it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.21: 0.9968 best possible recall, 3.99 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.275/0.753-mean/best, past_thr=0.522-mean: 15,12, 25,18, 37,30, 55,50, 172,191, 175,402, 435,178, 300,309, 432,428\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.69G    0.05763    0.01243          0         33        640: 100% 114/114 [00:33<00:00,  3.44it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.69G    0.04737    0.01219          0         84        640: 100% 114/114 [00:31<00:00,  3.66it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.69G    0.04787      0.012          0         53        640: 100% 114/114 [00:34<00:00,  3.26it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.69G    0.04783    0.01241          0         25        640: 100% 114/114 [00:30<00:00,  3.71it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.69G    0.04788    0.01264          0         82        640: 100% 114/114 [00:31<00:00,  3.66it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.69G    0.04552    0.01221          0         23        640: 100% 114/114 [00:30<00:00,  3.71it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.69G    0.04447    0.01196          0         82        640: 100% 114/114 [00:30<00:00,  3.68it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.69G    0.04352    0.01155          0         30        640: 100% 114/114 [00:30<00:00,  3.69it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.69G    0.04316    0.01172          0         55        640: 100% 114/114 [00:30<00:00,  3.70it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.69G    0.04158    0.01139          0         42        640: 100% 114/114 [00:31<00:00,  3.67it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.47it/s]\n",
      "                   all         80        419      0.333      0.253      0.198     0.0716\n",
      "\n",
      "10 epochs completed in 0.088 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m11 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.33272,              0.25298,              0.19756,             0.071553,              0.07455,             0.024709,                    0,              0.01335,                 0.01,              0.82845,              0.00059,               3.1958,              0.86067,              0.09543,              0.05162,              0.50471,              0.99833,              0.90884,               1.0558,                  0.2,               4.6619,                    0,              0.01093,              0.89837,              0.27939,                    0,               0.1101,              0.66407,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.9955\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01251, lrf=0.01, momentum=0.82921, weight_decay=0.00054, warmup_epochs=2.83534, warmup_momentum=0.86067, warmup_bias_lr=0.09871, box=0.05018, cls=0.57228, cls_pw=0.89742, obj=0.97697, obj_pw=1.00061, iou_t=0.2, anchor_t=4.12854, fl_gamma=0.0, hsv_h=0.01073, hsv_s=0.82755, hsv_v=0.30861, degrees=0.0, translate=0.10617, scale=0.68261, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.9661, mixup=0.0, copy_paste=0.0, anchors=2.9574\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.9574\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01251) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00054), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 112.13it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.59 anchors/target, 0.181 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7521: 100% 1000/1000 [00:02<00:00, 367.34it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.24: 0.9972 best possible recall, 3.79 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.274/0.753-mean/best, past_thr=0.533-mean: 15,12, 24,18, 38,29, 58,52, 166,195, 387,130, 173,395, 308,306, 415,424\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.69G    0.05679    0.01245          0         45        640: 100% 114/114 [00:32<00:00,  3.51it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.69G    0.04446     0.0121          0         58        640: 100% 114/114 [00:30<00:00,  3.75it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.69G    0.04462    0.01183          0         76        640: 100% 114/114 [00:30<00:00,  3.75it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.69G    0.04631    0.01267          0         82        640: 100% 114/114 [00:29<00:00,  3.82it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.69G    0.04538    0.01258          0         36        640: 100% 114/114 [00:30<00:00,  3.75it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.69G     0.0435    0.01171          0         43        640: 100% 114/114 [00:30<00:00,  3.77it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.69G    0.04063    0.01116          0         75        640: 100% 114/114 [00:29<00:00,  3.82it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.69G    0.04046    0.01111          0         47        640: 100% 114/114 [00:33<00:00,  3.39it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.69G    0.04072    0.01109          0         62        640: 100% 114/114 [00:30<00:00,  3.79it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.69G    0.03995    0.01152          0         48        640: 100% 114/114 [00:30<00:00,  3.77it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
      "                   all         80        419      0.369      0.243      0.216     0.0764\n",
      "\n",
      "10 epochs completed in 0.086 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m12 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.36912,              0.24344,              0.21594,             0.076431,             0.053666,             0.024782,                    0,              0.01251,                 0.01,              0.82921,              0.00054,               2.8353,              0.86067,              0.09871,              0.05018,              0.57228,              0.89742,              0.97697,               1.0006,                  0.2,               4.1285,                    0,              0.01073,              0.82755,              0.30861,                    0,              0.10617,              0.68261,                    0,                    0,                    0,                  0.5,               0.9661,                    0,                    0,               2.9574\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01099, lrf=0.01026, momentum=0.96327, weight_decay=0.00051, warmup_epochs=2.99674, warmup_momentum=0.83047, warmup_bias_lr=0.10161, box=0.04633, cls=0.59929, cls_pw=0.96584, obj=0.98037, obj_pw=1.14119, iou_t=0.2, anchor_t=3.77808, fl_gamma=0.0, hsv_h=0.01568, hsv_s=0.66507, hsv_v=0.403, degrees=0.0, translate=0.09701, scale=0.55552, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.9587, mixup=0.0, copy_paste=0.0, anchors=2.89294\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.89294\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01099) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00051), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 110.11it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.47 anchors/target, 0.146 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7520: 100% 1000/1000 [00:03<00:00, 325.86it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.26: 0.9953 best possible recall, 3.65 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.274/0.753-mean/best, past_thr=0.546-mean: 14,11, 24,18, 36,29, 55,49, 161,185, 452,154, 181,408, 302,304, 430,443\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.69G    0.05065    0.01269          0         29        640: 100% 114/114 [00:35<00:00,  3.25it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.69G    0.03897    0.01231          0         31        640: 100% 114/114 [00:33<00:00,  3.42it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.69G    0.04077    0.01207          0         34        640: 100% 114/114 [00:33<00:00,  3.37it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.69G     0.0407    0.01271          0         53        640: 100% 114/114 [00:33<00:00,  3.39it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.69G    0.04316    0.01344          0         44        640: 100% 114/114 [00:37<00:00,  3.01it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.69G     0.0422    0.01381          0         59        640: 100% 114/114 [00:33<00:00,  3.43it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.69G    0.04115    0.01298          0         48        640: 100% 114/114 [00:33<00:00,  3.42it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.69G     0.0409    0.01281          0         83        640: 100% 114/114 [00:33<00:00,  3.43it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.69G     0.0395    0.01307          0         50        640: 100% 114/114 [00:33<00:00,  3.44it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.69G    0.03781    0.01276          0         39        640: 100% 114/114 [00:33<00:00,  3.43it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
      "                   all         80        419      0.352      0.234      0.196     0.0672\n",
      "\n",
      "10 epochs completed in 0.095 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m13 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.35174,              0.23389,              0.19635,             0.067189,             0.052477,             0.026075,                    0,              0.01099,              0.01026,              0.96327,              0.00051,               2.9967,              0.83047,              0.10161,              0.04633,              0.59929,              0.96584,              0.98037,               1.1412,                  0.2,               3.7781,                    0,              0.01568,              0.66507,                0.403,                    0,              0.09701,              0.55552,                    0,                    0,                    0,                  0.5,               0.9587,                    0,                    0,               2.8929\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01026, lrf=0.01, momentum=0.93751, weight_decay=0.0005, warmup_epochs=3.09705, warmup_momentum=0.80507, warmup_bias_lr=0.10196, box=0.0506, cls=0.49291, cls_pw=0.96968, obj=1.0187, obj_pw=0.9926, iou_t=0.2, anchor_t=4.10709, fl_gamma=0.0, hsv_h=0.01524, hsv_s=0.66247, hsv_v=0.3993, degrees=0.0, translate=0.09861, scale=0.49596, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.90084\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.90084\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/train/exp16/weights/best.pt\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01026) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 110.60it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.58 anchors/target, 0.178 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7522: 100% 1000/1000 [00:02<00:00, 361.60it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.24: 0.9972 best possible recall, 3.77 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.274/0.753-mean/best, past_thr=0.534-mean: 15,12, 24,18, 37,30, 57,52, 172,197, 393,138, 167,406, 307,310, 428,428\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.69G     0.0542     0.0121          0         26        640: 100% 114/114 [00:35<00:00,  3.19it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.69G    0.04219    0.01169          0         71        640: 100% 114/114 [00:38<00:00,  2.93it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.69G    0.04162    0.01164          0         38        640: 100% 114/114 [00:34<00:00,  3.35it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.69G    0.04365    0.01227          0         16        640: 100% 114/114 [00:33<00:00,  3.40it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.69G    0.04387    0.01298          0         78        640: 100% 114/114 [00:33<00:00,  3.39it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.69G    0.04204    0.01248          0         23        640: 100% 114/114 [00:33<00:00,  3.42it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.69G    0.04202    0.01234          0         83        640: 100% 114/114 [00:33<00:00,  3.39it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.69G     0.0413    0.01185          0         31        640: 100% 114/114 [00:33<00:00,  3.40it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.69G    0.04077    0.01178          0         55        640: 100% 114/114 [00:33<00:00,  3.36it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.69G    0.03744    0.01132          0         39        640: 100% 114/114 [00:38<00:00,  2.98it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.85it/s]\n",
      "                   all         80        419      0.331      0.272      0.214     0.0789\n",
      "\n",
      "10 epochs completed in 0.097 hours.\n",
      "Results saved to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m14 generations finished, current result:\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n",
      "\u001b[34m\u001b[1mevolve: \u001b[0m             0.33054,              0.27208,              0.21405,             0.078886,             0.055416,             0.026784,                    0,              0.01026,                 0.01,              0.93751,               0.0005,                3.097,              0.80507,              0.10196,               0.0506,              0.49291,              0.96968,               1.0187,               0.9926,                  0.2,               4.1071,                    0,              0.01524,              0.66247,               0.3993,                    0,              0.09861,              0.49596,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.9008\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0153, lrf=0.01016, momentum=0.8522, weight_decay=0.00032, warmup_epochs=2.19086, warmup_momentum=0.95, warmup_bias_lr=0.11452, box=0.04409, cls=0.67126, cls_pw=1.0545, obj=0.89186, obj_pw=0.94544, iou_t=0.2, anchor_t=4.47515, fl_gamma=0.0, hsv_h=0.01426, hsv_s=0.9, hsv_v=0.34495, degrees=0.0, translate=0.09214, scale=0.65052, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "Overriding model.yaml anchors with anchors=2.0\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7016932 parameters, 7016932 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 342/349 items from runs/train/exp16/weights/best.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0153) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00032), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/train.cache... 1823 images, 977 backgrounds, 0 corrupt: 100% 1823/1823 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.1GB ram): 100% 1823/1823 [00:16<00:00, 110.58it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/FYP1YOLOv5/train_data/labels/val.cache... 80 images, 26 backgrounds, 0 corrupt: 100% 80/80 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.08 anchors/target, 0.027 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 4665 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7196: 100% 1000/1000 [00:02<00:00, 385.34it/s]\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.22: 0.9949 best possible recall, 2.83 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=640, metric_all=0.298/0.720-mean/best, past_thr=0.542-mean: 15,12, 28,21, 46,40, 218,285, 355,224, 401,411\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/evolve/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/evolve/exp2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.67G    0.06329     0.0219          0         33        640: 100% 114/114 [00:36<00:00,  3.13it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      3.67G    0.05051    0.01333          0         83        640: 100% 114/114 [00:34<00:00,  3.26it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      3.67G    0.04824    0.01236          0         53        640: 100% 114/114 [00:34<00:00,  3.27it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      3.67G    0.04655    0.01227          0         24        640: 100% 114/114 [00:35<00:00,  3.25it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      3.67G     0.0439    0.01244          0         82        640: 100% 114/114 [00:34<00:00,  3.27it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      3.67G    0.04206    0.01205          0         24        640: 100% 114/114 [00:38<00:00,  2.96it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      3.67G    0.04114    0.01184          0         82        640: 100% 114/114 [00:34<00:00,  3.32it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      3.67G    0.03999     0.0113          0         30        640: 100% 114/114 [00:35<00:00,  3.23it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      3.67G    0.03955    0.01147          0         56        640: 100% 114/114 [00:34<00:00,  3.32it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      3.67G    0.03735    0.01113          0         54        640:  65% 74/114 [00:22<00:12,  3.30it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 634, in <module>\n",
      "    main(opt)\n",
      "  File \"train.py\", line 609, in main\n",
      "    results = train(hyp.copy(), opt, device, callbacks)\n",
      "  File \"train.py\", line 309, in train\n",
      "    pred = model(imgs)  # forward\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/content/gdrive/MyDrive/FYP1YOLOv5/yolov5/models/yolo.py\", line 209, in forward\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train.py --epochs 10 --data custom_data.yaml --weights runs/train/exp16/weights/pre_trained_YOLO.pt --cache --evolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tunning  YOLO Model\n",
    "\n",
    "Training YOLOv5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Changing the Arhitecture of YOLO by freezing layers or changing Anchor & Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Generation based learning i.e., letting weights learn over epoch training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Hyperparameters Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exporting the Fine Tunned YOLO model weight file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate the file 'runs/train/exp16/weights/fine_tunned_YOLO.pt' . It contains the weights that would be later on used for drawing infrences ."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "16b0c8aa6e0f427e8a54d3791abb7504": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f7df330663048998adcf8a45bc8f69b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e896e6096dd244c59d7955e2035cd729",
       "IPY_MODEL_a6ff238c29984b24bf6d0bd175c19430",
       "IPY_MODEL_3c085ba3f3fd4c3c8a6bb41b41ce1479"
      ],
      "layout": "IPY_MODEL_16b0c8aa6e0f427e8a54d3791abb7504"
     }
    },
    "3c085ba3f3fd4c3c8a6bb41b41ce1479": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df554fb955c7454696beac5a82889386",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_74e9112a87a242f4831b7d68c7da6333",
      "value": " 780M/780M [00:05&lt;00:00, 126MB/s]"
     }
    },
    "6a27e43b0e434edd82ee63f0a91036ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "74e9112a87a242f4831b7d68c7da6333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6ff238c29984b24bf6d0bd175c19430": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cce0e6c0c4ec442cb47e65c674e02e92",
      "max": 818322941,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5b9f38e2f0d4f9aa97fe87265263743",
      "value": 818322941
     }
    },
    "c5b9f38e2f0d4f9aa97fe87265263743": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c7b2dd0f78384cad8e400b282996cdf5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cce0e6c0c4ec442cb47e65c674e02e92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df554fb955c7454696beac5a82889386": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e896e6096dd244c59d7955e2035cd729": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7b2dd0f78384cad8e400b282996cdf5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6a27e43b0e434edd82ee63f0a91036ca",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
